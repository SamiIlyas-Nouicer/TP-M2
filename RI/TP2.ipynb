{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sami/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk as nltk\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "folder_path = \"./Collection\"\n",
    "result_path = \"./result3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   porter = PorterStemmer()\n",
    "    lancaster = LancasterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    descriptors = []\n",
    "    inverses = {}\n",
    "\n",
    "    for doc_num, filename in enumerate(os.listdir(folder_path), start=1):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                text = file.read().lower()  \n",
    "\n",
    "                tokens = text.split() \n",
    "                tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "\n",
    "                for term in tokens:\n",
    "                    descriptors.append(f\"{doc_num} {term}\")\n",
    "                    inverses.setdefault(term, set()).add(doc_num)\n",
    "\n",
    "    # with open(os.path.join(result_path, \"descriptorsSplit.txt\"), 'w', encoding='utf-8') as desc_file:\n",
    "    #     desc_file.write(\"\\n\".join(descriptors))\n",
    "\n",
    "    # with open(os.path.join(result_path, \"inversesSplit.txt\"), 'w', encoding='utf-8') as inv_file:\n",
    "    #     for term, doc_nums in sorted(inverses.items()):\n",
    "    #         for doc_num in sorted(doc_nums):\n",
    "    #             inv_file.write(f\"{term} {doc_num}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llms: 15\n",
      "models: 12\n",
      "based: 11\n",
      "query: 11\n",
      "llm: 9\n",
      "using: 9\n",
      "language: 8\n",
      "ranking: 7\n",
      "large: 6\n",
      "tasks: 6\n",
      "text: 6\n",
      "information: 6\n",
      "recommendation: 6\n",
      "user: 6\n",
      "prompt: 6\n",
      "propose: 6\n",
      "results: 6\n",
      "reformulation: 6\n",
      "retrieval: 6\n",
      "10: 6\n",
      "graph: 6\n",
      "improve: 5\n",
      "efficiency: 5\n",
      "search: 5\n",
      "generate: 5\n",
      "performance: 5\n",
      "ndcg: 5\n",
      "descriptions: 5\n",
      "dataset: 5\n",
      "time: 4\n",
      "task: 4\n",
      "set: 4\n",
      "approach: 4\n",
      "shot: 4\n",
      "prompting: 4\n",
      "strategies: 4\n",
      "feedback: 4\n",
      "documents: 4\n",
      "diverse: 4\n",
      "paper: 4\n",
      "prp: 4\n",
      "symbolic: 4\n",
      "modeling: 3\n",
      "limited: 3\n",
      "long: 3\n",
      "process: 3\n",
      "item: 3\n",
      "ids: 3\n",
      "template: 3\n",
      "fine: 3\n",
      "inference: 3\n",
      "training: 3\n",
      "datasets: 3\n",
      "significantly: 3\n",
      "techniques: 3\n",
      "used: 3\n",
      "zero: 3\n",
      "multiple: 3\n",
      "introduce: 3\n",
      "previous: 3\n",
      "state: 3\n",
      "art: 3\n",
      "recent: 3\n",
      "intents: 3\n",
      "gencrf: 3\n",
      "capture: 3\n",
      "queries: 3\n",
      "first: 3\n",
      "model: 3\n",
      "benchmark: 3\n",
      "description: 3\n",
      "open: 3\n",
      "movie: 3\n",
      "names: 3\n",
      "achieve: 3\n",
      "session: 3\n",
      "various: 2\n",
      "e: 2\n",
      "input: 2\n",
      "could: 2\n",
      "take: 2\n",
      "may: 2\n",
      "systems: 2\n",
      "usually: 2\n",
      "discrete: 2\n",
      "understand: 2\n",
      "given: 2\n",
      "need: 2\n",
      "bridge: 2\n",
      "words: 2\n",
      "power: 2\n",
      "reduce: 2\n",
      "also: 2\n",
      "demonstrate: 2\n",
      "effectiveness: 2\n",
      "sequential: 2\n",
      "top: 2\n",
      "researchers: 2\n",
      "qr: 2\n",
      "better: 2\n",
      "ability: 2\n",
      "ensemble: 2\n",
      "technique: 2\n",
      "genqrensemble: 2\n",
      "leverages: 2\n",
      "instruction: 2\n",
      "genqrensemblerf: 2\n",
      "pseudo: 2\n",
      "relevant: 2\n",
      "ir: 2\n",
      "benchmarks: 2\n",
      "relative: 2\n",
      "improvements: 2\n",
      "5: 2\n",
      "mrr: 2\n",
      "well: 2\n",
      "problem: 2\n",
      "methods: 2\n",
      "capturing: 2\n",
      "generative: 2\n",
      "framework: 2\n",
      "generated: 2\n",
      "prompts: 2\n",
      "furthermore: 2\n",
      "novel: 2\n",
      "evaluation: 2\n",
      "beir: 2\n",
      "obtained: 2\n",
      "web: 2\n",
      "gpt: 2\n",
      "like: 2\n",
      "alpaca: 2\n",
      "natural: 2\n",
      "use: 2\n",
      "detailed: 2\n",
      "goodreads: 2\n",
      "sourced: 2\n",
      "scraped: 2\n",
      "metrics: 2\n",
      "generation: 2\n",
      "pointwise: 2\n",
      "literature: 2\n",
      "parameters: 2\n",
      "blackbox: 2\n",
      "commercial: 2\n",
      "4: 2\n",
      "solutions: 2\n",
      "outperforms: 2\n",
      "supervised: 2\n",
      "semantic: 2\n",
      "approaches: 2\n",
      "grammar: 2\n",
      "textual: 2\n",
      "learning: 2\n",
      "grained: 2\n",
      "manifested: 1\n",
      "unparalleled: 1\n",
      "capability: 1\n",
      "g: 1\n",
      "multi: 1\n",
      "step: 1\n",
      "reasoning: 1\n",
      "mostly: 1\n",
      "plain: 1\n",
      "contain: 1\n",
      "noisy: 1\n",
      "thus: 1\n",
      "efficient: 1\n",
      "enough: 1\n",
      "recommender: 1\n",
      "require: 1\n",
      "immediate: 1\n",
      "response: 1\n",
      "filled: 1\n",
      "allow: 1\n",
      "extensive: 1\n",
      "tuning: 1\n",
      "unleash: 1\n",
      "address: 1\n",
      "problems: 1\n",
      "distill: 1\n",
      "specific: 1\n",
      "continuous: 1\n",
      "vectors: 1\n",
      "design: 1\n",
      "strategy: 1\n",
      "attempt: 1\n",
      "experimental: 1\n",
      "three: 1\n",
      "real: 1\n",
      "world: 1\n",
      "distillation: 1\n",
      "pod: 1\n",
      "n: 1\n",
      "although: 1\n",
      "improved: 1\n",
      "improvement: 1\n",
      "finding: 1\n",
      "inspire: 1\n",
      "community: 1\n",
      "transform: 1\n",
      "original: 1\n",
      "aligns: 1\n",
      "intent: 1\n",
      "improves: 1\n",
      "experience: 1\n",
      "recently: 1\n",
      "shown: 1\n",
      "promising: 1\n",
      "due: 1\n",
      "exploit: 1\n",
      "knowledge: 1\n",
      "inherent: 1\n",
      "taking: 1\n",
      "inspiration: 1\n",
      "success: 1\n",
      "benefited: 1\n",
      "many: 1\n",
      "investigate: 1\n",
      "help: 1\n",
      "context: 1\n",
      "paraphrases: 1\n",
      "sets: 1\n",
      "keywords: 1\n",
      "ultimately: 1\n",
      "improving: 1\n",
      "post: 1\n",
      "variant: 1\n",
      "incorporate: 1\n",
      "evaluations: 1\n",
      "four: 1\n",
      "find: 1\n",
      "generates: 1\n",
      "reformulations: 1\n",
      "18: 1\n",
      "map: 1\n",
      "upto: 1\n",
      "24: 1\n",
      "msmarco: 1\n",
      "passage: 1\n",
      "shows: 1\n",
      "gains: 1\n",
      "relevance: 1\n",
      "9: 1\n",
      "known: 1\n",
      "aimed: 1\n",
      "enhancing: 1\n",
      "single: 1\n",
      "successful: 1\n",
      "completion: 1\n",
      "rate: 1\n",
      "automatically: 1\n",
      "modifying: 1\n",
      "leverage: 1\n",
      "often: 1\n",
      "redundant: 1\n",
      "expansions: 1\n",
      "potentially: 1\n",
      "constraining: 1\n",
      "clustering: 1\n",
      "intentions: 1\n",
      "adaptively: 1\n",
      "differentiated: 1\n",
      "phase: 1\n",
      "variable: 1\n",
      "initial: 1\n",
      "customized: 1\n",
      "clusters: 1\n",
      "groups: 1\n",
      "distinctly: 1\n",
      "represent: 1\n",
      "explores: 1\n",
      "combine: 1\n",
      "innovative: 1\n",
      "weighted: 1\n",
      "aggregation: 1\n",
      "optimize: 1\n",
      "crucially: 1\n",
      "integrates: 1\n",
      "rewarding: 1\n",
      "qerm: 1\n",
      "refine: 1\n",
      "loops: 1\n",
      "empirical: 1\n",
      "experiments: 1\n",
      "achieves: 1\n",
      "surpassing: 1\n",
      "sotas: 1\n",
      "12: 1\n",
      "adapted: 1\n",
      "boosting: 1\n",
      "retriever: 1\n",
      "advancing: 1\n",
      "field: 1\n",
      "plays: 1\n",
      "pivotal: 1\n",
      "role: 1\n",
      "providing: 1\n",
      "concise: 1\n",
      "informative: 1\n",
      "summaries: 1\n",
      "captivate: 1\n",
      "potential: 1\n",
      "viewers: 1\n",
      "essential: 1\n",
      "traditionally: 1\n",
      "manual: 1\n",
      "scraping: 1\n",
      "consuming: 1\n",
      "susceptible: 1\n",
      "data: 1\n",
      "inconsistencies: 1\n",
      "years: 1\n",
      "3: 1\n",
      "source: 1\n",
      "emerged: 1\n",
      "powerful: 1\n",
      "tools: 1\n",
      "processing: 1\n",
      "explored: 1\n",
      "items: 1\n",
      "conduct: 1\n",
      "study: 1\n",
      "movielens: 1\n",
      "1m: 1\n",
      "comprising: 1\n",
      "titles: 1\n",
      "consisting: 1\n",
      "books: 1\n",
      "subsequently: 1\n",
      "prompted: 1\n",
      "considering: 1\n",
      "features: 1\n",
      "cast: 1\n",
      "directors: 1\n",
      "ml: 1\n",
      "author: 1\n",
      "publisher: 1\n",
      "compared: 1\n",
      "combination: 1\n",
      "hits: 1\n",
      "demonstrated: 1\n",
      "exhibits: 1\n",
      "significant: 1\n",
      "promise: 1\n",
      "comparable: 1\n",
      "ones: 1\n",
      "directly: 1\n",
      "feeding: 1\n",
      "candidate: 1\n",
      "interesting: 1\n",
      "practical: 1\n",
      "however: 1\n",
      "found: 1\n",
      "difficult: 1\n",
      "outperform: 1\n",
      "tuned: 1\n",
      "baseline: 1\n",
      "rankers: 1\n",
      "analyze: 1\n",
      "listwise: 1\n",
      "existing: 1\n",
      "argue: 1\n",
      "shelf: 1\n",
      "fully: 1\n",
      "challenging: 1\n",
      "formulations: 1\n",
      "burden: 1\n",
      "new: 1\n",
      "called: 1\n",
      "pairwise: 1\n",
      "standard: 1\n",
      "moderate: 1\n",
      "sized: 1\n",
      "trec: 1\n",
      "dl: 1\n",
      "2019: 1\n",
      "2020: 1\n",
      "flan: 1\n",
      "ul2: 1\n",
      "20b: 1\n",
      "performs: 1\n",
      "favorably: 1\n",
      "best: 1\n",
      "50x: 1\n",
      "estimated: 1\n",
      "size: 1\n",
      "outperforming: 1\n",
      "instructgpt: 1\n",
      "175b: 1\n",
      "seven: 1\n",
      "baselines: 1\n",
      "chatgpt: 1\n",
      "solution: 1\n",
      "2: 1\n",
      "average: 1\n",
      "several: 1\n",
      "variants: 1\n",
      "show: 1\n",
      "possible: 1\n",
      "competitive: 1\n",
      "even: 1\n",
      "linear: 1\n",
      "complexity: 1\n",
      "involves: 1\n",
      "series: 1\n",
      "interactive: 1\n",
      "actions: 1\n",
      "fulfill: 1\n",
      "complex: 1\n",
      "current: 1\n",
      "typically: 1\n",
      "prioritize: 1\n",
      "deep: 1\n",
      "understanding: 1\n",
      "overlooking: 1\n",
      "structure: 1\n",
      "interactions: 1\n",
      "focus: 1\n",
      "structural: 1\n",
      "generalized: 1\n",
      "representation: 1\n",
      "neglecting: 1\n",
      "word: 1\n",
      "level: 1\n",
      "ranker: 1\n",
      "sgr: 1\n",
      "aims: 1\n",
      "advantage: 1\n",
      "leveraging: 1\n",
      "concretely: 1\n",
      "rules: 1\n",
      "convert: 1\n",
      "allows: 1\n",
      "integrating: 1\n",
      "history: 1\n",
      "interaction: 1\n",
      "seamlessly: 1\n",
      "inputs: 1\n",
      "moreover: 1\n",
      "discrepancy: 1\n",
      "pre: 1\n",
      "trained: 1\n",
      "corpora: 1\n",
      "produce: 1\n",
      "objective: 1\n",
      "enhance: 1\n",
      "structures: 1\n",
      "within: 1\n",
      "format: 1\n",
      "self: 1\n",
      "including: 1\n",
      "link: 1\n",
      "prediction: 1\n",
      "node: 1\n",
      "content: 1\n",
      "contrastive: 1\n",
      "enable: 1\n",
      "topological: 1\n",
      "coarse: 1\n",
      "experiment: 1\n",
      "comprehensive: 1\n",
      "analysis: 1\n",
      "two: 1\n",
      "aol: 1\n",
      "tiangong: 1\n",
      "st: 1\n",
      "confirm: 1\n",
      "superiority: 1\n",
      "paradigm: 1\n",
      "offers: 1\n",
      "effective: 1\n",
      "methodology: 1\n",
      "bridges: 1\n",
      "gap: 1\n",
      "traditional: 1\n",
      "modern: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def count_tokens_in_folder(folder_path):\n",
    "    token_counts = Counter()\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                tokens = re.findall(r'\\w+', text.lower())  \n",
    "                tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "                token_counts.update(tokens)\n",
    "\n",
    "    return token_counts\n",
    "\n",
    "\n",
    "token_counts = count_tokens_in_folder(folder_path)\n",
    "\n",
    "for token, count in token_counts.most_common():\n",
    "    print(f\"{token}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF scores for file: D3.txt\n",
      "large: 0.000000\n",
      "language: 0.000000\n",
      "models: 0.000000\n",
      "llm: 0.046958\n",
      "have: 0.011739\n",
      "manifested: 0.051877\n",
      "unparalleled: 0.051877\n",
      "modeling: 0.031808\n",
      "capability: 0.051877\n",
      "on: 0.000000\n",
      "various: 0.031808\n",
      "tasks: 0.010557\n",
      "e: 0.103754\n",
      "g: 0.051877\n",
      "multi: 0.051877\n",
      "step: 0.051877\n",
      "reasoning: 0.051877\n",
      "but: 0.063616\n",
      "the: 0.000000\n",
      "input: 0.031808\n",
      "to: 0.000000\n",
      "these: 0.040137\n",
      "is: 0.000000\n",
      "mostly: 0.051877\n",
      "limited: 0.063616\n",
      "plain: 0.051877\n",
      "text: 0.040137\n",
      "which: 0.005279\n",
      "could: 0.103754\n",
      "be: 0.060206\n",
      "very: 0.051877\n",
      "long: 0.155630\n",
      "and: 0.000000\n",
      "contain: 0.051877\n",
      "noisy: 0.051877\n",
      "information: 0.020069\n",
      "take: 0.031808\n",
      "time: 0.040137\n",
      "process: 0.020069\n",
      "thus: 0.051877\n",
      "may: 0.103754\n",
      "not: 0.031808\n",
      "efficient: 0.051877\n",
      "enough: 0.051877\n",
      "for: 0.015836\n",
      "recommender: 0.051877\n",
      "systems: 0.031808\n",
      "that: 0.000000\n",
      "require: 0.051877\n",
      "immediate: 0.051877\n",
      "response: 0.051877\n",
      "in: 0.000000\n",
      "based: 0.000000\n",
      "recommendation: 0.159040\n",
      "user: 0.023479\n",
      "item: 0.063616\n",
      "ids: 0.155630\n",
      "are: 0.020069\n",
      "usually: 0.103754\n",
      "filled: 0.051877\n",
      "a: 0.000000\n",
      "template: 0.063616\n",
      "i: 0.051877\n",
      "discrete: 0.103754\n",
      "prompt: 0.127232\n",
      "allow: 0.051877\n",
      "understand: 0.031808\n",
      "given: 0.031808\n",
      "task: 0.040137\n",
      "need: 0.031808\n",
      "extensive: 0.051877\n",
      "fine: 0.020069\n",
      "tuning: 0.051877\n",
      "bridge: 0.103754\n",
      "words: 0.103754\n",
      "unleash: 0.051877\n",
      "power: 0.031808\n",
      "of: 0.000000\n",
      "address: 0.051877\n",
      "problems: 0.051877\n",
      "we: 0.000000\n",
      "propose: 0.005279\n",
      "distill: 0.051877\n",
      "specific: 0.051877\n",
      "set: 0.020069\n",
      "continuous: 0.051877\n",
      "vectors: 0.051877\n",
      "so: 0.051877\n",
      "as: 0.011739\n",
      "reduce: 0.031808\n",
      "inference: 0.155630\n",
      "also: 0.031808\n",
      "design: 0.051877\n",
      "training: 0.155630\n",
      "strategy: 0.051877\n",
      "with: 0.005279\n",
      "an: 0.011739\n",
      "attempt: 0.051877\n",
      "improve: 0.023479\n",
      "efficiency: 0.127232\n",
      "experimental: 0.051877\n",
      "results: 0.011739\n",
      "three: 0.051877\n",
      "real: 0.051877\n",
      "world: 0.051877\n",
      "datasets: 0.020069\n",
      "demonstrate: 0.031808\n",
      "effectiveness: 0.031808\n",
      "our: 0.020069\n",
      "distillation: 0.051877\n",
      "pod: 0.051877\n",
      "approach: 0.011739\n",
      "both: 0.031808\n",
      "sequential: 0.031808\n",
      "top: 0.031808\n",
      "n: 0.051877\n",
      "although: 0.051877\n",
      "can: 0.011739\n",
      "significantly: 0.020069\n",
      "improved: 0.051877\n",
      "improvement: 0.051877\n",
      "this: 0.000000\n",
      "finding: 0.051877\n",
      "inspire: 0.051877\n",
      "researchers: 0.031808\n",
      "community: 0.051877\n",
      "further: 0.031808\n",
      "\n",
      "TF-IDF scores for file: D1.txt\n",
      "query: 0.112886\n",
      "reformulation: 0.119280\n",
      "qr: 0.194538\n",
      "is: 0.000000\n",
      "a: 0.000000\n",
      "set: 0.037629\n",
      "of: 0.000000\n",
      "techniques: 0.037629\n",
      "used: 0.037629\n",
      "to: 0.000000\n",
      "transform: 0.097269\n",
      "user: 0.044023\n",
      "s: 0.075257\n",
      "original: 0.097269\n",
      "search: 0.075257\n",
      "text: 0.037629\n",
      "that: 0.000000\n",
      "better: 0.194538\n",
      "aligns: 0.097269\n",
      "with: 0.019795\n",
      "the: 0.000000\n",
      "intent: 0.097269\n",
      "and: 0.000000\n",
      "improves: 0.097269\n",
      "their: 0.059640\n",
      "experience: 0.097269\n",
      "recently: 0.097269\n",
      "zero: 0.291807\n",
      "shot: 0.178920\n",
      "has: 0.059640\n",
      "been: 0.097269\n",
      "shown: 0.097269\n",
      "be: 0.037629\n",
      "promising: 0.097269\n",
      "approach: 0.022011\n",
      "due: 0.097269\n",
      "its: 0.194538\n",
      "ability: 0.059640\n",
      "exploit: 0.097269\n",
      "knowledge: 0.097269\n",
      "inherent: 0.097269\n",
      "in: 0.000000\n",
      "large: 0.000000\n",
      "language: 0.000000\n",
      "models: 0.000000\n",
      "by: 0.009898\n",
      "taking: 0.097269\n",
      "inspiration: 0.097269\n",
      "from: 0.037629\n",
      "success: 0.097269\n",
      "ensemble: 0.194538\n",
      "prompting: 0.075257\n",
      "strategies: 0.037629\n",
      "which: 0.019795\n",
      "have: 0.022011\n",
      "benefited: 0.097269\n",
      "many: 0.097269\n",
      "tasks: 0.009898\n",
      "we: 0.000000\n",
      "investigate: 0.097269\n",
      "if: 0.097269\n",
      "they: 0.059640\n",
      "can: 0.022011\n",
      "help: 0.097269\n",
      "improve: 0.022011\n",
      "this: 0.000000\n",
      "context: 0.097269\n",
      "propose: 0.009898\n",
      "an: 0.022011\n",
      "based: 0.000000\n",
      "technique: 0.059640\n",
      "genqrensemble: 0.194538\n",
      "leverages: 0.059640\n",
      "paraphrases: 0.097269\n",
      "instruction: 0.059640\n",
      "generate: 0.037629\n",
      "multiple: 0.037629\n",
      "sets: 0.097269\n",
      "keywords: 0.097269\n",
      "ultimately: 0.097269\n",
      "improving: 0.097269\n",
      "retrieval: 0.119280\n",
      "performance: 0.037629\n",
      "further: 0.059640\n",
      "introduce: 0.059640\n",
      "post: 0.097269\n",
      "variant: 0.097269\n",
      "genqrensemblerf: 0.194538\n",
      "incorporate: 0.097269\n",
      "pseudo: 0.194538\n",
      "relevant: 0.194538\n",
      "feedback: 0.178920\n",
      "on: 0.000000\n",
      "evaluations: 0.097269\n",
      "over: 0.119280\n",
      "four: 0.097269\n",
      "ir: 0.059640\n",
      "benchmarks: 0.059640\n",
      "find: 0.097269\n",
      "generates: 0.097269\n",
      "reformulations: 0.097269\n",
      "relative: 0.194538\n",
      "ndcg: 0.044023\n",
      "10: 0.075257\n",
      "improvements: 0.194538\n",
      "up: 0.059640\n",
      "18: 0.097269\n",
      "map: 0.097269\n",
      "upto: 0.097269\n",
      "24: 0.097269\n",
      "previous: 0.037629\n",
      "state: 0.037629\n",
      "art: 0.037629\n",
      "msmarco: 0.097269\n",
      "passage: 0.097269\n",
      "ranking: 0.059640\n",
      "task: 0.037629\n",
      "shows: 0.097269\n",
      "gains: 0.097269\n",
      "5: 0.059640\n",
      "mrr: 0.059640\n",
      "using: 0.019795\n",
      "relevance: 0.097269\n",
      "9: 0.097269\n",
      "documents: 0.037629\n",
      "\n",
      "TF-IDF scores for file: D4.txt\n",
      "query: 0.234134\n",
      "reformulation: 0.212054\n",
      "is: 0.000000\n",
      "a: 0.000000\n",
      "well: 0.172923\n",
      "known: 0.086461\n",
      "problem: 0.053013\n",
      "in: 0.000000\n",
      "information: 0.066896\n",
      "retrieval: 0.212054\n",
      "ir: 0.053013\n",
      "aimed: 0.086461\n",
      "at: 0.086461\n",
      "enhancing: 0.086461\n",
      "single: 0.086461\n",
      "search: 0.033448\n",
      "successful: 0.086461\n",
      "completion: 0.086461\n",
      "rate: 0.086461\n",
      "by: 0.017596\n",
      "automatically: 0.086461\n",
      "modifying: 0.086461\n",
      "user: 0.019566\n",
      "s: 0.033448\n",
      "input: 0.053013\n",
      "recent: 0.033448\n",
      "methods: 0.053013\n",
      "leverage: 0.086461\n",
      "large: 0.000000\n",
      "language: 0.000000\n",
      "models: 0.000000\n",
      "llms: 0.058697\n",
      "to: 0.000000\n",
      "improve: 0.019566\n",
      "but: 0.053013\n",
      "often: 0.086461\n",
      "generate: 0.066896\n",
      "limited: 0.053013\n",
      "and: 0.000000\n",
      "redundant: 0.086461\n",
      "expansions: 0.086461\n",
      "potentially: 0.086461\n",
      "constraining: 0.086461\n",
      "their: 0.053013\n",
      "effectiveness: 0.053013\n",
      "capturing: 0.053013\n",
      "diverse: 0.345845\n",
      "intents: 0.259384\n",
      "this: 0.000000\n",
      "paper: 0.019566\n",
      "we: 0.000000\n",
      "propose: 0.008798\n",
      "gencrf: 0.259384\n",
      "generative: 0.053013\n",
      "clustering: 0.086461\n",
      "framework: 0.172923\n",
      "capture: 0.053013\n",
      "intentions: 0.086461\n",
      "adaptively: 0.086461\n",
      "based: 0.000000\n",
      "on: 0.000000\n",
      "multiple: 0.033448\n",
      "differentiated: 0.086461\n",
      "generated: 0.053013\n",
      "queries: 0.106027\n",
      "the: 0.000000\n",
      "phase: 0.086461\n",
      "for: 0.008798\n",
      "first: 0.033448\n",
      "time: 0.033448\n",
      "leverages: 0.053013\n",
      "variable: 0.086461\n",
      "from: 0.033448\n",
      "initial: 0.086461\n",
      "using: 0.008798\n",
      "customized: 0.086461\n",
      "prompts: 0.053013\n",
      "then: 0.053013\n",
      "clusters: 0.086461\n",
      "them: 0.086461\n",
      "into: 0.033448\n",
      "groups: 0.086461\n",
      "distinctly: 0.086461\n",
      "represent: 0.086461\n",
      "furthermore: 0.053013\n",
      "explores: 0.086461\n",
      "combine: 0.086461\n",
      "with: 0.008798\n",
      "innovative: 0.086461\n",
      "weighted: 0.086461\n",
      "aggregation: 0.086461\n",
      "strategies: 0.033448\n",
      "optimize: 0.086461\n",
      "performance: 0.100343\n",
      "crucially: 0.086461\n",
      "integrates: 0.086461\n",
      "novel: 0.053013\n",
      "evaluation: 0.053013\n",
      "rewarding: 0.086461\n",
      "model: 0.053013\n",
      "qerm: 0.086461\n",
      "refine: 0.086461\n",
      "process: 0.033448\n",
      "through: 0.053013\n",
      "feedback: 0.053013\n",
      "loops: 0.086461\n",
      "empirical: 0.086461\n",
      "experiments: 0.086461\n",
      "beir: 0.053013\n",
      "benchmark: 0.033448\n",
      "demonstrate: 0.053013\n",
      "that: 0.000000\n",
      "achieves: 0.086461\n",
      "state: 0.033448\n",
      "of: 0.000000\n",
      "art: 0.033448\n",
      "surpassing: 0.086461\n",
      "previous: 0.033448\n",
      "sotas: 0.086461\n",
      "up: 0.053013\n",
      "12: 0.086461\n",
      "ndcg: 0.019566\n",
      "10: 0.033448\n",
      "these: 0.033448\n",
      "techniques: 0.033448\n",
      "can: 0.019566\n",
      "be: 0.033448\n",
      "adapted: 0.086461\n",
      "various: 0.053013\n",
      "significantly: 0.033448\n",
      "boosting: 0.086461\n",
      "retriever: 0.086461\n",
      "advancing: 0.086461\n",
      "field: 0.086461\n",
      "\n",
      "TF-IDF scores for file: D6.txt\n",
      "the: 0.000000\n",
      "description: 0.155630\n",
      "of: 0.000000\n",
      "an: 0.023479\n",
      "item: 0.031808\n",
      "plays: 0.051877\n",
      "a: 0.000000\n",
      "pivotal: 0.051877\n",
      "role: 0.051877\n",
      "in: 0.000000\n",
      "providing: 0.051877\n",
      "concise: 0.051877\n",
      "and: 0.000000\n",
      "informative: 0.051877\n",
      "summaries: 0.051877\n",
      "to: 0.000000\n",
      "captivate: 0.051877\n",
      "potential: 0.051877\n",
      "viewers: 0.051877\n",
      "is: 0.000000\n",
      "essential: 0.051877\n",
      "for: 0.021115\n",
      "recommendation: 0.031808\n",
      "systems: 0.031808\n",
      "traditionally: 0.051877\n",
      "such: 0.063616\n",
      "descriptions: 0.259384\n",
      "were: 0.051877\n",
      "obtained: 0.103754\n",
      "through: 0.031808\n",
      "manual: 0.051877\n",
      "web: 0.103754\n",
      "scraping: 0.051877\n",
      "techniques: 0.020069\n",
      "which: 0.005279\n",
      "are: 0.020069\n",
      "time: 0.020069\n",
      "consuming: 0.051877\n",
      "susceptible: 0.051877\n",
      "data: 0.051877\n",
      "inconsistencies: 0.051877\n",
      "recent: 0.020069\n",
      "years: 0.051877\n",
      "large: 0.000000\n",
      "language: 0.000000\n",
      "models: 0.000000\n",
      "llms: 0.035218\n",
      "as: 0.035218\n",
      "gpt: 0.031808\n",
      "3: 0.051877\n",
      "5: 0.031808\n",
      "open: 0.063616\n",
      "source: 0.051877\n",
      "like: 0.103754\n",
      "alpaca: 0.103754\n",
      "have: 0.035218\n",
      "emerged: 0.051877\n",
      "powerful: 0.051877\n",
      "tools: 0.051877\n",
      "natural: 0.031808\n",
      "processing: 0.051877\n",
      "tasks: 0.005279\n",
      "this: 0.000000\n",
      "paper: 0.011739\n",
      "we: 0.000000\n",
      "explored: 0.051877\n",
      "how: 0.051877\n",
      "can: 0.011739\n",
      "use: 0.031808\n",
      "generate: 0.040137\n",
      "detailed: 0.103754\n",
      "items: 0.051877\n",
      "conduct: 0.051877\n",
      "study: 0.051877\n",
      "used: 0.020069\n",
      "movielens: 0.051877\n",
      "1m: 0.051877\n",
      "dataset: 0.259384\n",
      "comprising: 0.051877\n",
      "movie: 0.155630\n",
      "titles: 0.051877\n",
      "goodreads: 0.103754\n",
      "consisting: 0.051877\n",
      "names: 0.155630\n",
      "books: 0.051877\n",
      "subsequently: 0.051877\n",
      "sourced: 0.031808\n",
      "llm: 0.023479\n",
      "was: 0.103754\n",
      "prompted: 0.051877\n",
      "with: 0.015836\n",
      "few: 0.051877\n",
      "shot: 0.031808\n",
      "prompting: 0.020069\n",
      "on: 0.000000\n",
      "considering: 0.051877\n",
      "multiple: 0.020069\n",
      "features: 0.051877\n",
      "cast: 0.051877\n",
      "directors: 0.051877\n",
      "ml: 0.051877\n",
      "author: 0.051877\n",
      "publisher: 0.051877\n",
      "generated: 0.031808\n",
      "then: 0.031808\n",
      "compared: 0.051877\n",
      "scraped: 0.103754\n",
      "using: 0.005279\n",
      "combination: 0.051877\n",
      "top: 0.031808\n",
      "hits: 0.051877\n",
      "mrr: 0.031808\n",
      "ndcg: 0.011739\n",
      "evaluation: 0.031808\n",
      "metrics: 0.031808\n",
      "results: 0.023479\n",
      "demonstrated: 0.051877\n",
      "that: 0.000000\n",
      "based: 0.000000\n",
      "generation: 0.031808\n",
      "exhibits: 0.051877\n",
      "significant: 0.051877\n",
      "promise: 0.051877\n",
      "comparable: 0.051877\n",
      "ones: 0.051877\n",
      "by: 0.005279\n",
      "\n",
      "TF-IDF scores for file: D2.txt\n",
      "ranking: 0.220210\n",
      "documents: 0.046312\n",
      "using: 0.024363\n",
      "large: 0.000000\n",
      "language: 0.000000\n",
      "models: 0.000000\n",
      "llms: 0.054182\n",
      "by: 0.042636\n",
      "directly: 0.059858\n",
      "feeding: 0.059858\n",
      "the: 0.000000\n",
      "query: 0.023156\n",
      "and: 0.000000\n",
      "candidate: 0.059858\n",
      "into: 0.023156\n",
      "prompt: 0.073403\n",
      "is: 0.000000\n",
      "an: 0.013545\n",
      "interesting: 0.059858\n",
      "practical: 0.059858\n",
      "problem: 0.036702\n",
      "however: 0.059858\n",
      "researchers: 0.036702\n",
      "have: 0.013545\n",
      "found: 0.059858\n",
      "it: 0.119716\n",
      "difficult: 0.059858\n",
      "to: 0.000000\n",
      "outperform: 0.059858\n",
      "fine: 0.023156\n",
      "tuned: 0.059858\n",
      "baseline: 0.059858\n",
      "rankers: 0.059858\n",
      "on: 0.000000\n",
      "benchmark: 0.023156\n",
      "datasets: 0.023156\n",
      "we: 0.000000\n",
      "analyze: 0.059858\n",
      "pointwise: 0.119716\n",
      "listwise: 0.059858\n",
      "prompts: 0.036702\n",
      "used: 0.023156\n",
      "existing: 0.059858\n",
      "methods: 0.036702\n",
      "argue: 0.059858\n",
      "that: 0.000000\n",
      "off: 0.059858\n",
      "shelf: 0.059858\n",
      "do: 0.059858\n",
      "not: 0.036702\n",
      "fully: 0.059858\n",
      "understand: 0.036702\n",
      "these: 0.023156\n",
      "challenging: 0.059858\n",
      "formulations: 0.059858\n",
      "in: 0.000000\n",
      "this: 0.000000\n",
      "paper: 0.013545\n",
      "propose: 0.012182\n",
      "significantly: 0.023156\n",
      "reduce: 0.036702\n",
      "burden: 0.059858\n",
      "a: 0.000000\n",
      "new: 0.059858\n",
      "technique: 0.036702\n",
      "called: 0.059858\n",
      "pairwise: 0.059858\n",
      "prompting: 0.023156\n",
      "prp: 0.239431\n",
      "our: 0.023156\n",
      "results: 0.027091\n",
      "are: 0.023156\n",
      "first: 0.023156\n",
      "literature: 0.119716\n",
      "achieve: 0.073403\n",
      "state: 0.023156\n",
      "of: 0.000000\n",
      "art: 0.023156\n",
      "performance: 0.023156\n",
      "standard: 0.059858\n",
      "benchmarks: 0.036702\n",
      "moderate: 0.059858\n",
      "sized: 0.059858\n",
      "open: 0.036702\n",
      "sourced: 0.036702\n",
      "trec: 0.059858\n",
      "dl: 0.059858\n",
      "2019: 0.059858\n",
      "2020: 0.059858\n",
      "based: 0.000000\n",
      "flan: 0.059858\n",
      "ul2: 0.059858\n",
      "model: 0.073403\n",
      "with: 0.018273\n",
      "20b: 0.059858\n",
      "parameters: 0.119716\n",
      "performs: 0.059858\n",
      "favorably: 0.059858\n",
      "previous: 0.023156\n",
      "best: 0.059858\n",
      "approach: 0.013545\n",
      "which: 0.012182\n",
      "blackbox: 0.119716\n",
      "commercial: 0.119716\n",
      "gpt: 0.036702\n",
      "4: 0.119716\n",
      "has: 0.073403\n",
      "50x: 0.059858\n",
      "estimated: 0.059858\n",
      "size: 0.059858\n",
      "while: 0.036702\n",
      "outperforming: 0.059858\n",
      "other: 0.059858\n",
      "llm: 0.027091\n",
      "solutions: 0.119716\n",
      "such: 0.036702\n",
      "as: 0.013545\n",
      "instructgpt: 0.059858\n",
      "175b: 0.059858\n",
      "over: 0.036702\n",
      "10: 0.069468\n",
      "for: 0.006091\n",
      "all: 0.059858\n",
      "metrics: 0.036702\n",
      "same: 0.059858\n",
      "template: 0.036702\n",
      "seven: 0.059858\n",
      "beir: 0.036702\n",
      "tasks: 0.006091\n",
      "outperforms: 0.119716\n",
      "supervised: 0.036702\n",
      "baselines: 0.059858\n",
      "chatgpt: 0.059858\n",
      "solution: 0.059858\n",
      "2: 0.059858\n",
      "more: 0.059858\n",
      "than: 0.059858\n",
      "average: 0.059858\n",
      "ndcg: 0.013545\n",
      "furthermore: 0.036702\n",
      "several: 0.059858\n",
      "variants: 0.059858\n",
      "improve: 0.013545\n",
      "efficiency: 0.036702\n",
      "show: 0.059858\n",
      "possible: 0.059858\n",
      "competitive: 0.059858\n",
      "even: 0.059858\n",
      "linear: 0.059858\n",
      "complexity: 0.059858\n",
      "\n",
      "TF-IDF scores for file: D5.txt\n",
      "session: 0.233445\n",
      "search: 0.060206\n",
      "involves: 0.077815\n",
      "a: 0.000000\n",
      "series: 0.077815\n",
      "of: 0.000000\n",
      "interactive: 0.077815\n",
      "queries: 0.047712\n",
      "and: 0.000000\n",
      "actions: 0.077815\n",
      "to: 0.000000\n",
      "fulfill: 0.077815\n",
      "user: 0.017609\n",
      "s: 0.030103\n",
      "complex: 0.077815\n",
      "information: 0.090309\n",
      "need: 0.047712\n",
      "current: 0.077815\n",
      "strategies: 0.060206\n",
      "typically: 0.077815\n",
      "prioritize: 0.077815\n",
      "sequential: 0.047712\n",
      "modeling: 0.095424\n",
      "for: 0.023754\n",
      "deep: 0.077815\n",
      "semantic: 0.155630\n",
      "understanding: 0.077815\n",
      "overlooking: 0.077815\n",
      "the: 0.000000\n",
      "graph: 0.466891\n",
      "structure: 0.077815\n",
      "in: 0.000000\n",
      "interactions: 0.077815\n",
      "while: 0.047712\n",
      "some: 0.077815\n",
      "approaches: 0.155630\n",
      "focus: 0.077815\n",
      "on: 0.000000\n",
      "capturing: 0.047712\n",
      "structural: 0.077815\n",
      "they: 0.047712\n",
      "use: 0.047712\n",
      "generalized: 0.077815\n",
      "representation: 0.077815\n",
      "documents: 0.030103\n",
      "neglecting: 0.077815\n",
      "word: 0.077815\n",
      "level: 0.077815\n",
      "this: 0.000000\n",
      "paper: 0.017609\n",
      "we: 0.000000\n",
      "propose: 0.007918\n",
      "symbolic: 0.311261\n",
      "ranker: 0.077815\n",
      "sgr: 0.077815\n",
      "which: 0.007918\n",
      "aims: 0.077815\n",
      "take: 0.047712\n",
      "advantage: 0.077815\n",
      "both: 0.047712\n",
      "text: 0.090309\n",
      "based: 0.000000\n",
      "by: 0.007918\n",
      "leveraging: 0.077815\n",
      "power: 0.047712\n",
      "recent: 0.030103\n",
      "large: 0.000000\n",
      "language: 0.000000\n",
      "models: 0.000000\n",
      "llms: 0.088046\n",
      "concretely: 0.077815\n",
      "first: 0.030103\n",
      "introduce: 0.095424\n",
      "set: 0.060206\n",
      "grammar: 0.155630\n",
      "rules: 0.077815\n",
      "convert: 0.077815\n",
      "into: 0.030103\n",
      "allows: 0.077815\n",
      "integrating: 0.077815\n",
      "history: 0.077815\n",
      "interaction: 0.077815\n",
      "process: 0.030103\n",
      "task: 0.030103\n",
      "instruction: 0.047712\n",
      "seamlessly: 0.077815\n",
      "as: 0.017609\n",
      "inputs: 0.077815\n",
      "llm: 0.017609\n",
      "moreover: 0.077815\n",
      "given: 0.047712\n",
      "natural: 0.047712\n",
      "discrepancy: 0.077815\n",
      "between: 0.155630\n",
      "pre: 0.077815\n",
      "trained: 0.077815\n",
      "textual: 0.155630\n",
      "corpora: 0.077815\n",
      "produce: 0.077815\n",
      "using: 0.007918\n",
      "our: 0.120412\n",
      "objective: 0.077815\n",
      "is: 0.000000\n",
      "enhance: 0.077815\n",
      "ability: 0.047712\n",
      "capture: 0.095424\n",
      "structures: 0.077815\n",
      "within: 0.077815\n",
      "format: 0.077815\n",
      "achieve: 0.047712\n",
      "self: 0.077815\n",
      "supervised: 0.047712\n",
      "learning: 0.155630\n",
      "tasks: 0.007918\n",
      "including: 0.077815\n",
      "link: 0.077815\n",
      "prediction: 0.077815\n",
      "node: 0.077815\n",
      "content: 0.077815\n",
      "generation: 0.047712\n",
      "generative: 0.047712\n",
      "contrastive: 0.077815\n",
      "enable: 0.077815\n",
      "topological: 0.077815\n",
      "from: 0.030103\n",
      "coarse: 0.077815\n",
      "grained: 0.155630\n",
      "fine: 0.030103\n",
      "experiment: 0.077815\n",
      "results: 0.017609\n",
      "comprehensive: 0.077815\n",
      "analysis: 0.077815\n",
      "two: 0.077815\n",
      "benchmark: 0.030103\n",
      "datasets: 0.030103\n",
      "aol: 0.077815\n",
      "tiangong: 0.077815\n",
      "st: 0.077815\n",
      "confirm: 0.077815\n",
      "superiority: 0.077815\n",
      "approach: 0.017609\n",
      "paradigm: 0.077815\n",
      "also: 0.047712\n",
      "offers: 0.077815\n",
      "novel: 0.047712\n",
      "effective: 0.077815\n",
      "methodology: 0.077815\n",
      "that: 0.000000\n",
      "bridges: 0.077815\n",
      "gap: 0.077815\n",
      "traditional: 0.077815\n",
      "modern: 0.077815\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import re\n",
    "import math\n",
    "\n",
    "def calculate_tf_idf(folder_path):\n",
    "    token_counts_per_file = []\n",
    "    document_frequencies = Counter()\n",
    "    total_documents = 0\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        if os.path.isfile(file_path):\n",
    "            total_documents += 1\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                # Tokenization: Using regex to split on non-word characters\n",
    "                tokens = re.findall(r'\\w+', text.lower())\n",
    "                token_count = Counter(tokens)\n",
    "                max_frequency = max(token_count.values())\n",
    "\n",
    "                # Save token count and document frequencies for TF and IDF\n",
    "                token_counts_per_file.append({\n",
    "                    'file': filename,\n",
    "                    'token_count': token_count,\n",
    "                    'max_frequency': max_frequency\n",
    "                })\n",
    "\n",
    "                # Update document frequency for each token (i.e., number of documents that contain the word)\n",
    "                for token in set(tokens):  # Use `set` to avoid multiple counts in the same document\n",
    "                    document_frequencies[token] += 1\n",
    "\n",
    "    # Calculate TF-IDF for each word in each document\n",
    "    tf_idf_scores = {}\n",
    "\n",
    "    for file_data in token_counts_per_file:\n",
    "        file_name = file_data['file']\n",
    "        tf_idf_scores[file_name] = {}\n",
    "\n",
    "        for token, count in file_data['token_count'].items():\n",
    "            # Calculate TF (Term Frequency)\n",
    "            tf = count / file_data['max_frequency']\n",
    "\n",
    "            # Calculate IDF (Inverse Document Frequency)\n",
    "            idf = math.log10(total_documents / document_frequencies[token])\n",
    "\n",
    "            # Calculate TF-IDF\n",
    "            tf_idf_scores[file_name][token] = tf * idf\n",
    "\n",
    "    return tf_idf_scores\n",
    "\n",
    "# Specify the folder path\n",
    "\n",
    "# Calculate TF-IDF across all files in the folder\n",
    "tf_idf_scores = calculate_tf_idf(folder_path)\n",
    "\n",
    "# Display TF-IDF results\n",
    "for file, tokens in tf_idf_scores.items():\n",
    "    print(f\"TF-IDF scores for file: {file}\")\n",
    "    for token, score in tokens.items():\n",
    "        print(f\"{token}: {score:.6f}\")\n",
    "    print()  # Blank line between files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for D3.txt has been saved to D3.txt_tf_idf.csv\n",
      "Data for D1.txt has been saved to D1.txt_tf_idf.csv\n",
      "Data for D4.txt has been saved to D4.txt_tf_idf.csv\n",
      "Data for D6.txt has been saved to D6.txt_tf_idf.csv\n",
      "Data for D2.txt has been saved to D2.txt_tf_idf.csv\n",
      "Data for D5.txt has been saved to D5.txt_tf_idf.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_tf_idf_for_files(folder_path):\n",
    "    # Step 1: Gather token counts and document frequencies across all files\n",
    "    token_counts_per_file = []\n",
    "    document_frequencies = Counter()\n",
    "    total_documents = 0\n",
    "\n",
    "    # Iterate over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Check if it's a file\n",
    "        if os.path.isfile(file_path):\n",
    "            total_documents += 1\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                # Tokenization\n",
    "                tokens = text.split() \n",
    "                tokens = [token for token in tokens if token not in stop_words]\n",
    "                tokens = [porter.stem(token) for token in tokens]\n",
    "\n",
    "                token_count = Counter(tokens)\n",
    "                max_frequency = max(token_count.values())\n",
    "\n",
    "                # Save token count and max frequency for TF\n",
    "                token_counts_per_file.append({\n",
    "                    'file': filename,\n",
    "                    'token_count': token_count,\n",
    "                    'max_frequency': max_frequency\n",
    "                })\n",
    "\n",
    "                # Update document frequency for IDF (count documents containing each token)\n",
    "                for token in set(tokens):  # Use set to avoid double counting in the same document\n",
    "                    document_frequencies[token] += 1\n",
    "\n",
    "    # Step 2: Calculate TF, IDF, and TF-IDF for each file\n",
    "    for file_data in token_counts_per_file:\n",
    "        file_name = file_data['file']\n",
    "        token_count = file_data['token_count']\n",
    "        max_frequency = file_data['max_frequency']\n",
    "\n",
    "        # Prepare CSV data\n",
    "        csv_data = []\n",
    "        csv_header = ['Token', 'Fréquence du terme', 'Fréquence max', 'Nombre de documents dans la collection', 'Nombre de documents contenant le terme', 'log10', 'Poids du terme (TF-IDF)']\n",
    "\n",
    "        for token, count in token_count.items():\n",
    "            # Calculate TF (Term Frequency)\n",
    "            tf = count / max_frequency\n",
    "\n",
    "            # Calculate IDF (Inverse Document Frequency)\n",
    "            idf = math.log10(total_documents / document_frequencies[token])\n",
    "\n",
    "            # Calculate TF-IDF (Weight)\n",
    "            tf_idf = tf * idf\n",
    "\n",
    "            # Prepare row data for CSV\n",
    "            csv_data.append([token, count, max_frequency, total_documents, document_frequencies[token], idf, tf_idf])\n",
    "\n",
    "        # Step 3: Write each file's data to its own CSV\n",
    "        output_csv = f\"{file_name}_tf_idf.csv\"\n",
    "        with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(csv_header)  # Write header\n",
    "            writer.writerows(csv_data)   # Write token data\n",
    "\n",
    "        print(f\"Data for {file_name} has been saved to {output_csv}\")\n",
    "\n",
    "# Specify the folder path\n",
    "\n",
    "# Process the folder and generate CSVs\n",
    "calculate_tf_idf_for_files(folder_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
